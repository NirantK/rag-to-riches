{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mermaid import Mermaid\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we trying to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-596fb351-e3f9-4c8b-a416-aaaef5549cd9\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = 'flowchart LR\\nA[LLM Invocations Synthetic/Human Inputs] --> B[Unit Tests]\\nA --> C[Logging Traces]\\nB & C --> D[Eval & Curation 1. Human review 2. Model-based 3. A/B tests]\\nD -->|Automate this more over time| E[Fine Tune/ Curated Data]\\nD --> F[Prompt Eng]\\nE & F -->|Versioned Together| G[Improve Model]\\nG --> A';\n",
       "            const element = document.querySelector('.mermaid-596fb351-e3f9-4c8b-a416-aaaef5549cd9');\n",
       "            const { svg } = await mermaid.render('graphDiv-596fb351-e3f9-4c8b-a416-aaaef5549cd9', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x1057d5e90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mermaid(\"flowchart LR\\nA[LLM Invocations Synthetic/Human Inputs] --> B[Unit Tests]\\nA --> C[Logging Traces]\\nB & C --> D[Eval & Curation 1. Human review 2. Model-based 3. A/B tests]\\nD -->|Automate this more over time| E[Fine Tune/ Curated Data]\\nD --> F[Prompt Eng]\\nE & F -->|Versioned Together| G[Improve Model]\\nG --> A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume there exists python library inspired by DSPy. The ideals of DSPy are:\n",
    "1. Modular LLM Programs\n",
    "2. Scoring/evaluation\n",
    "3. Optimization\n",
    "\n",
    "## What is a prompt?\n",
    "1. Prompt Instruction -> \"Based on the context provided, answer the question\" #BayesianSignatureOpt does in DSPy\n",
    "2. Few Shot Examples: Selection, Metadata, Re-ordering -> RAG Evaluation -> ragas #Compilation, labeled demos -> MIPRO\n",
    "3. Reference information to answer the current question -> ragas # Retriever \n",
    "4. Teaching the LLM to use the Examples well. Synthetic Data e.g. reasoning about the question and answer -> Synthetic Data Generation\n",
    "\n",
    "Option 1: `question, context -> answer`\n",
    "Option 2: `question, context, intermediate_reasoning, reasoning -> final_answer` ✅\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pool_of_demos -> selected_demos\n",
    "?? (missing inputs) -> labeled_demos \n",
    "labeled_demos, selected_demos ->few_shot_examples \n",
    "retrieval/search -> reference\n",
    "few_shot_examples, reference -> context\n",
    "question, context -> intermediate_reasoning\n",
    "question, context, intermediate_reasoning -> reasoning \n",
    "question, context, reasoning -> final_answer\n",
    "\"\"\"\n",
    "\n",
    "## What are the main components of this library?\n",
    "1. Some way to capture relationships between inputs and outputs. If there is an intermediate output, we should optimize for that too ✅  \n",
    "2. Some way to \"score\" the output -> Loss Function (composition of functions) ✅ -> Score using faithfulness from ragas (1-Hallucination)\n",
    "3. Some way to use this score, to update the steps in between -> Backpropagation / Hyperparameter Optimization (hyperopt) \"Update\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    name: str = \"question\"\n",
    "    value: str = Field(..., description=\"The question to be answered\")\n",
    "\n",
    "class Context(BaseModel):\n",
    "    name: str = \"context\"\n",
    "    value: str = Field(..., description=\"The context to be used to answer the question\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    name: str = \"response\"\n",
    "    value: str = Field(..., description=\"The response to the question\")\n",
    "\n",
    "class StringAnswer(Response):\n",
    "    name: str = \"answer\"\n",
    "    value: str = Field(..., description=\"The answer to the question\")\n",
    "\n",
    "class NumberAnswer(Response):\n",
    "    name: str = \"answer\"\n",
    "    value: Union[float, int] = Field(..., description=\"The answer to the question\")\n",
    "\n",
    "class ListAnswer(Response):\n",
    "    name: str = \"answer\"\n",
    "    value: List[str] = Field(..., description=\"The answer to the question\")\n",
    "\n",
    "class IntermediateReasoning(BaseModel):\n",
    "    name: str = \"intermediate_reasoning\"\n",
    "    value: str = Field(..., description=\"The intermediate reasoning to be used to answer the question\")\n",
    "\n",
    "class Reasoning(BaseModel):\n",
    "    name: str = \"reasoning\"\n",
    "    value: str = Field(..., description=\"The reasoning to be used to answer the question\")\n",
    "\n",
    "\"\"\"Scenario 1: question, context -> answer\"\"\" # function A\n",
    "def question_context_to_answer(**kwargs) -> StringAnswer:\n",
    "    \"\"\"\n",
    "    This function takes a question and a context, and returns an answer.\n",
    "    If the system does return a list, it will now be read as a string or expected to be a string. \n",
    "    \"\"\"\n",
    "    question, context = kwargs['question'], kwargs['context']\n",
    "    return StringAnswer(value=f\"The answer to the question {question}, {context}\")\n",
    "\n",
    "\"\"\"Scenario 2: question, context, reasoning -> final_answer\"\"\" # Function B i.e. B(A)\n",
    "def question_context_with_intermediate_reasoning(**kwargs) -> StringAnswer:\n",
    "    \"\"\"\n",
    "    This function takes a question and a context, and returns an answer.\n",
    "    If the system does return a list, it will now be read as a string or expected to be a string. \n",
    "    \"\"\"\n",
    "    reasoning = Reasoning(value=f\"The reasoning to be used to answer the question: {kwargs['question']}, based on {kwargs['context']}\")\n",
    "    context = Context(value=f\"The context to be used to answer the question: {kwargs['context']} and uses the reasoning: {reasoning.value}\")\n",
    "    return question_context_to_answer(question=kwargs[\"question\"], context=context)\n",
    "\n",
    "\"\"\"Scenario 3: question, context, intermediate_reasoning, reasoning -> final_answer\"\"\" # Function C\n",
    "def question_context_w_reasoning(question: Question, context: Context) -> StringAnswer:\n",
    "    \"\"\"\n",
    "    This function takes a question and a context, and returns an answer.\n",
    "    If the system does return a list, it will now be read as a string or expected to be a string. \n",
    "    \"\"\"\n",
    "    intermediate_reasoning = Reasoning(...)\n",
    "    reasoning = question_context_with_intermediate_reasoning(...)\n",
    "    updated_context = context + reasoning\n",
    "    return question_context_to_answer(question, updated_context)\n",
    "\n",
    "\"\"\"Scenario 3: \n",
    "question, context -> intermediate_reasoning\n",
    "question, context, intermediate_reasoning -> reasoning \n",
    "question, context, reasoning -> final_answer\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graph\n",
    "\n",
    "# Copy what PyTorch got right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\"\"\"Scenario 2: question, context, reasoning -> final_answer\"\"\" # Function B i.e. B(A)\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions based on the context.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    reasoning = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "class Reasoning(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    reasoning = dspy.OutputField()\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, collection_name=\"rag_contexts\", num_passages=10):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages) # function\n",
    "        self.generate_reasoning = dspy.Predict(Reasoning)\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        reason = self.generate_reasoning(question=question, context=context)\n",
    "        prediction = self.generate_answer(context=context, question=question, reasoning=reason)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nkpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's create a new python library inspired by DSPy. The ideals of DSPy are:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1. Modular LLM Programs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 2. Scoring/eval\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 3. Optimization -> Prompt Instruction, Example Selection, Synthetic Data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnkpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MIPRO\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnkpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Evaluate\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgold_passages_retrieved\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gold_passages_retrieved\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nkpy'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# from nkpy.compilers import MIPRO\n",
    "# from nkpy.evaluate.evaluate import Evaluate\n",
    "# from metrics.gold_passages_retrieved import gold_passages_retrieved\n",
    "# from models.simplified_baleen import SimplifiedBaleen\n",
    "# from metrics.validate_context_and_answer_and_hops import validate_context_and_answer_and_hops\n",
    "\n",
    "# # Assume devset, trainset, uncompiled_baleen, and compiled_baleen are defined\n",
    "\n",
    "# # Initialize the evaluate function\n",
    "# evaluate_on_hotpotqa = Evaluate(\n",
    "#     devset=devset,\n",
    "#     num_threads=1,\n",
    "#     display_progress=True,\n",
    "#     display_table=5\n",
    "# )\n",
    "\n",
    "# # Evaluate the uncompiled Baleen model\n",
    "# uncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(\n",
    "#     uncompiled_baleen,\n",
    "#     metric=gold_passages_retrieved,\n",
    "#     display=False\n",
    "# )\n",
    "\n",
    "# # Evaluate the compiled Baleen model\n",
    "# compiled_baleen_retrieval_score = evaluate_on_hotpotqa(\n",
    "#     compiled_baleen,\n",
    "#     metric=gold_passages_retrieved\n",
    "# )\n",
    "\n",
    "# print(f\"## Retrieval Score for uncompiled Baleen: {uncompiled_baleen_retrieval_score}\")\n",
    "# print(f\"## Retrieval Score for compiled Baleen: {compiled_baleen_retrieval_score}\")\n",
    "\n",
    "# # Compile the model using MIPRO\n",
    "# compiler = MIPRO(metric=validate_context_and_answer_and_hops)\n",
    "# compiled_baleen = compiler.compile(\n",
    "#     SimplifiedBaleen(),\n",
    "#     teacher=SimplifiedBaleen(passages_per_hop=2),\n",
    "#     trainset=trainset\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIPRO Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-53550a41-e411-4bc8-a4d0-4f3a0d3d5846\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = 'graph LR\\nA[Start] --> B[Initialize]\\nB --> C[Propose]\\nC --> D[Update]\\nD --> E{Extract Optimized Sets?}\\nE -->|Yes| F[Evaluate Best Candidates]\\nE -->|No| C\\nF --> G{All Trials Completed?}\\nG -->|Yes| H[Return Optimal Assignment]\\nG -->|No| C';\n",
       "            const element = document.querySelector('.mermaid-53550a41-e411-4bc8-a4d0-4f3a0d3d5846');\n",
       "            const { svg } = await mermaid.render('graphDiv-53550a41-e411-4bc8-a4d0-4f3a0d3d5846', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x1168a5890>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mermaid(\"graph LR\\nA[Start] --> B[Initialize]\\nB --> C[Propose]\\nC --> D[Update]\\nD --> E{Extract Optimized Sets?}\\nE -->|Yes| F[Evaluate Best Candidates]\\nE -->|No| C\\nF --> G{All Trials Completed?}\\nG -->|Yes| H[Return Optimal Assignment]\\nG -->|No| C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIPRO \"Propose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-c4f94f9e-6c34-4b6f-8e5d-1ceda2e7bf00\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = 'graph TD\\nA[Start Propose] --> B[Prepare LLM Inputs]\\nB --> C[LLM Generates Candidate Instructions]\\nC --> D[Process & Rank LLM Output]\\nD --> E[TPE Samples Instructions]\\nE --> F[TPE Samples Demos]\\nF --> G[Create Partial Assignments]\\nG --> H[End Propose]';\n",
       "            const element = document.querySelector('.mermaid-c4f94f9e-6c34-4b6f-8e5d-1ceda2e7bf00');\n",
       "            const { svg } = await mermaid.render('graphDiv-c4f94f9e-6c34-4b6f-8e5d-1ceda2e7bf00', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x1057d71d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mermaid(\"graph TD\\nA[Start Propose] --> B[Prepare LLM Inputs]\\nB --> C[LLM Generates Candidate Instructions]\\nC --> D[Process & Rank LLM Output]\\nD --> E[TPE Samples Instructions]\\nE --> F[TPE Samples Demos]\\nF --> G[Create Partial Assignments]\\nG --> H[End Propose]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragchallenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
