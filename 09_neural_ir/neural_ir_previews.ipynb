{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'prithivida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'},\n",
       "  'model_file': 'model.onnx'},\n",
       " {'model': 'prithvida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'},\n",
       "  'model_file': 'model.onnx'},\n",
       " {'model': 'Qdrant/bm42-all-minilm-l6-v2-attentions',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Light sparse embedding model, which assigns an importance score to each token in the text',\n",
       "  'size_in_GB': 0.09,\n",
       "  'sources': {'hf': 'Qdrant/all_miniLM_L6_v2_with_attentions'},\n",
       "  'model_file': 'model.onnx',\n",
       "  'additional_files': ['stopwords.txt'],\n",
       "  'requires_idf': True},\n",
       " {'model': 'Qdrant/bm25',\n",
       "  'description': 'BM25 as sparse embeddings meant to be used with Qdrant',\n",
       "  'size_in_GB': 0.01,\n",
       "  'sources': {'hf': 'Qdrant/bm25'},\n",
       "  'model_file': 'mock.file',\n",
       "  'additional_files': ['arabic.txt',\n",
       "   'azerbaijani.txt',\n",
       "   'basque.txt',\n",
       "   'bengali.txt',\n",
       "   'catalan.txt',\n",
       "   'chinese.txt',\n",
       "   'danish.txt',\n",
       "   'dutch.txt',\n",
       "   'english.txt',\n",
       "   'finnish.txt',\n",
       "   'french.txt',\n",
       "   'german.txt',\n",
       "   'greek.txt',\n",
       "   'hebrew.txt',\n",
       "   'hinglish.txt',\n",
       "   'hungarian.txt',\n",
       "   'indonesian.txt',\n",
       "   'italian.txt',\n",
       "   'kazakh.txt',\n",
       "   'nepali.txt',\n",
       "   'norwegian.txt',\n",
       "   'portuguese.txt',\n",
       "   'romanian.txt',\n",
       "   'russian.txt',\n",
       "   'slovene.txt',\n",
       "   'spanish.txt',\n",
       "   'swedish.txt',\n",
       "   'tajik.txt',\n",
       "   'turkish.txt'],\n",
       "  'requires_idf': True}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding, SparseEmbedding, LateInteractionTextEmbedding\n",
    "from typing import List\n",
    "from tokenizers import Tokenizer\n",
    "from sentence_transformers import CrossEncoder\n",
    "SparseTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Hell yeah\",\n",
    "    \"I am not sure about that\",\n",
    "    \"I am good\",\n",
    "    \"Hmm, no\",\n",
    "    \"I wasn't going to do that\",\n",
    "]\n",
    "\n",
    "queries = [\"Yes\", \"No\", \"Maybe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044df5e02b214ad483bf74090e0d6879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423fb16a11ff4185b3c4e3d8a2ab74d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2a94db25144709ae911fc06deb9220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"prithivida/Splade_PP_en_v1\"\n",
    "# This triggers the model download\n",
    "model = SparseTextEmbedding(model_name=model_name)\n",
    "dense_embedding_model = TextEmbedding()\n",
    "late_interaction_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
    "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_document_embeddings_list: List = list(\n",
    "    late_interaction_model.embed(documents, batch_size=6)\n",
    ")\n",
    "colbert_query_embeddings_list: List = list(\n",
    "    late_interaction_model.embed(queries, batch_size=6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 128), (4, 128))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colbert_document_embeddings_list[0].shape, colbert_query_embeddings_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_document_embeddings_list: List = list(\n",
    "    dense_embedding_model.embed(documents, batch_size=6)\n",
    ")\n",
    "dense_query_embeddings_list: List = list(\n",
    "    dense_embedding_model.embed(queries, batch_size=6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384,), (384,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_document_embeddings_list[0].shape, dense_query_embeddings_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_document_embeddings_list: List[SparseEmbedding] = list(\n",
    "    model.embed(documents, batch_size=6)\n",
    ")\n",
    "sparse_query_embeddings_list: List[SparseEmbedding] = list(\n",
    "    model.embed(queries, batch_size=6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sparse_document_embeddings_list[0].indices), len(sparse_query_embeddings_list[0].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "tokenizer = Tokenizer.from_pretrained(\n",
    "    SparseTextEmbedding.list_supported_models()[0][\"sources\"][\"hf\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Hell yeah\n",
      "Representation: {\n",
      "  \"hell\": 2.937955617904663,\n",
      "  \"yeah\": 2.629483938217163,\n",
      "  \"yes\": 2.186044931411743,\n",
      "  \"fuck\": 1.5486094951629639,\n",
      "  \"heck\": 1.4398324489593506,\n",
      "  \"god\": 1.3642619848251343,\n",
      "  \"no\": 1.317369818687439,\n",
      "  \"shit\": 1.1750346422195435,\n",
      "  \"heaven\": 1.0905754566192627,\n",
      "  \"sure\": 0.9624558687210083,\n",
      "  \"damn\": 0.7844801545143127,\n",
      "  \"what\": 0.7418169379234314,\n",
      "  \"yep\": 0.5640624165534973,\n",
      "  \"religion\": 0.25662943720817566,\n",
      "  \"good\": 0.06601270288228989,\n",
      "  \"sex\": 0.02037149667739868\n",
      "}\n",
      "Dense Document Representation: Array of length 384\n",
      "Colbert Document Representation: Tensor of shape (11, 128)\n",
      "Query: Yes\n",
      "Representation: {\n",
      "  \"yes\": 2.6260664463043213,\n",
      "  \"yeah\": 1.5697352886199951,\n",
      "  \"no\": 1.314880609512329\n",
      "}\n",
      "Dense Query Representation: Array of length 384\n",
      "Colbert Query Representation: Tensor of shape (4, 128)\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_and_weights(sparse_embedding, tokenizer):\n",
    "    token_weight_dict = {}\n",
    "    for i in range(len(sparse_embedding.indices)):\n",
    "        token = tokenizer.decode([sparse_embedding.indices[i]])\n",
    "        weight = sparse_embedding.values[i]\n",
    "        token_weight_dict[token] = weight\n",
    "\n",
    "    # Sort the dictionary by weights\n",
    "    token_weight_dict = dict(\n",
    "        sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "    return token_weight_dict\n",
    "\n",
    "def get_colbert_scores(query_embedding: np.array, document_embeddings: np.array):\n",
    "    scores = np.matmul(query_embedding, document_embeddings.transpose(0, 2, 1))\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "# Test the function with the first SparseEmbedding\n",
    "print(f\"Document: {documents[index]}\")\n",
    "\n",
    "print(\n",
    "    \"Representation:\",\n",
    "    json.dumps(\n",
    "        get_tokens_and_weights(sparse_document_embeddings_list[index], tokenizer),\n",
    "        indent=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Dense Document Representation: Array of length {len(dense_document_embeddings_list[index])}\")\n",
    "print(f\"Colbert Document Representation: Tensor of shape {colbert_document_embeddings_list[index].shape}\")\n",
    "print(f\"Query: {queries[index]}\")\n",
    "\n",
    "print(\n",
    "    \"Representation:\",\n",
    "    json.dumps(\n",
    "        get_tokens_and_weights(sparse_query_embeddings_list[index], tokenizer), indent=2\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Dense Query Representation: Array of length {len(dense_query_embeddings_list[index])}\")\n",
    "print(f\"Colbert Query Representation: Tensor of shape {colbert_query_embeddings_list[index].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0a9678a5294e6487140eb89b9e4dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colbert_tokenizer = Tokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance_scores(query_embedding: np.array, document_embeddings: np.array, k: int):\n",
    "    scores = np.matmul(query_embedding, document_embeddings.transpose(0, 2, 1))\n",
    "\n",
    "    # Apply max-pooling across document terms (axis=2) to find the max similarity per query term\n",
    "    # Shape after max-pool: [num_documents, num_query_terms]\n",
    "    max_scores_per_query_term = np.max(scores, axis=2)\n",
    "\n",
    "    # Sum the scores across query terms to get the total score for each document\n",
    "    # Shape after sum: [num_documents]\n",
    "    total_scores = np.sum(max_scores_per_query_term, axis=1)\n",
    "\n",
    "    # Sort the documents based on their total scores and get the indices of the top-k documents\n",
    "    sorted_indices = np.argsort(total_scores)[::-1][:k]\n",
    "\n",
    "    return sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: No\n",
      "Document: I am not sure about that\n",
      "\n",
      "Token Interaction Scores:\n",
      "\n",
      "Query Token: no\n",
      "Doc Token: i: 0.5062\n",
      "Doc Token: am: 0.3855\n",
      "Doc Token: not: 0.4358\n",
      "Doc Token: sure: 0.3073\n",
      "Doc Token: about: 0.3195\n",
      "Doc Token: that: 0.3722\n"
     ]
    }
   ],
   "source": [
    "def get_colbert_token_scores(query, document, query_embedding, document_embedding, tokenizer):\n",
    "    # Tokenize query and document\n",
    "    query_tokens = tokenizer.encode(query).tokens\n",
    "    doc_tokens = tokenizer.encode(document).tokens\n",
    "    \n",
    "    # Remove CLS and SEP tokens\n",
    "    query_tokens = query_tokens[1:-1]  # Remove first and last token\n",
    "    doc_tokens = doc_tokens[1:-1]  # Remove first and last token\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = np.matmul(query_embedding[1:-1], document_embedding[1:-1].T)\n",
    "    \n",
    "    # Create a matrix of scores\n",
    "    score_matrix = []\n",
    "    for i, q_token in enumerate(query_tokens):\n",
    "        row = []\n",
    "        for j, d_token in enumerate(doc_tokens):\n",
    "            row.append((d_token, scores[i, j]))\n",
    "        score_matrix.append((q_token, row))\n",
    "    \n",
    "    return score_matrix\n",
    "\n",
    "# Example usage\n",
    "query_index = 1\n",
    "doc_index = 1\n",
    "\n",
    "query = queries[query_index]\n",
    "document = documents[doc_index]\n",
    "query_embedding = colbert_query_embeddings_list[query_index]\n",
    "document_embedding = colbert_document_embeddings_list[doc_index]\n",
    "\n",
    "token_scores = get_colbert_token_scores(query, document, query_embedding, document_embedding, colbert_tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Document: {document}\")\n",
    "print(\"\\nToken Interaction Scores:\")\n",
    "for q_token, d_scores in token_scores:\n",
    "    print(f\"\\nQuery Token: {q_token}\")\n",
    "    for d_token, score in d_scores:\n",
    "        print(f\"Doc Token: {d_token}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.1523714, -6.287037 ], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_encoder_model.predict([('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'), \n",
    "                        ('How many people live in Berlin?', 'Berlin is well known for its museums.')])\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragchallenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
